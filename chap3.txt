Scaling data and KNN Regression 

Scaling options
Scaling options:

min-max
standardization
median-MAD
map to arbitrary function (e.g. sigmoid, tanh) 

sklearn's scaler
from sklearn.preprocessing import scaler

sc = scaler()
scaled_train_features = sc.fit_transform(train_features)
scaled_test_features = sc.transform(test_features) 

Making subplots
# create figure and list containing axes
f, ax = plt.subplots(nrows=2, ncols=1)
# plot histograms of before and after scaling
train_features.iloc[:, 2].hist(ax=ax[0])
ax[1].hist(scaled_train_features[:, 2])
plt.show() 

Standardizing data
Some models, like K-nearest neighbors (KNN) & neural networks, work better with scaled data -- so we'll standardize our data.

We'll also remove unimportant variables (day of week), according to feature importances, by indexing the features DataFrames with .iloc[]. KNN uses distances to find similar points for predictions, so big features outweigh small ones. Scaling data fixes that.

sklearn's scale() will standardize data, which sets the mean to 0 and standard deviation to 1. Ideally we'd want to use StandardScaler with fit_transform() on the training data, and fit() on the test data, but we are limited to 15 lines of code here.

Once we've scaled the data, we'll check that it worked by plotting histograms of the data.

from sklearn.preprocessing import scale

# Remove unimportant features (weekdays)
train_features = train_features.iloc[:, :-4]
test_features = test_features.iloc[:, :-4]

# Standardize the train and test features
scaled_train_features = scale(train_features)
scaled_test_features = scale(test_features)

# Plot histograms of the 14-day SMA RSI before and after scaling
f, ax = plt.subplots(nrows=2, ncols=1)
train_features.iloc[:, 2].hist(ax=ax[0])
ax[1].hist(scaled_train_features[:, 2])
plt.show() 


Optimize n_neighbors
Now that we have scaled data, we can try using a KNN model. To maximize performance, we should tune our model's hyperparameters. For the k-nearest neighbors algorithm, we only have one hyperparameter: n, the number of neighbors. We set this hyperparameter when we create the model with KNeighborsRegressor. The argument for the number of neighbors is n_neighbors.

We want to try a range of values that passes through the setting with the best performance. Usually we will start with 2 neighbors, and increase until our scoring metric starts to decrease. We'll use the R2 value from the .score() method on the test set (scaled_test_features and test_targets) to optimize n here. We'll use the test set scores to determine the best n. 

from sklearn.neighbors import KNeighborsRegressor

for n in range(2, 13):
    # Create and fit the KNN model
    knn = KNeighborsRegressor(n_neighbors=n)
    
    # Fit the model to the training data
    knn.fit(scaled_train_features, train_targets)
    
    # Print number of neighbors and the score to find the best value of n
    print("n_neighbors =", n)
    print('train, test scores')
    print(knn.score(scaled_train_features, train_targets))
    print(knn.score(scaled_test_features, test_targets))
    print()  # prints a blank line 

n_neighbors = 2
train, test scores
0.7086590518110245
-0.24570512723742513

n_neighbors = 3
train, test scores
0.6161299695003466
-0.028247987527901145

n_neighbors = 4
train, test scores
0.5698590844708643
0.05406963728898184

n_neighbors = 5
train, test scores
0.5306669823361658
0.09562673296186885

n_neighbors = 6
train, test scores
0.4924157634083257
0.06493624818165344

n_neighbors = 7
train, test scores
0.4638477451246331
0.018590670460287284

n_neighbors = 8
train, test scores
0.4305841272960338
0.03430514088288383

n_neighbors = 9
train, test scores
0.39035273655318137
-0.05499368593229881

n_neighbors = 10
train, test scores
0.3585431044577594
-0.04569165134882858

n_neighbors = 11
train, test scores
0.31002206869733073
-0.08074815512838707

n_neighbors = 12
train, test scores
0.2742940406863563
-0.07302787030122526


Evaluate KNN performance
We just saw a few things with our KNN scores. For one, the training scores started high and decreased with increasing n, which is typical. The test set performance reached a peak at 5 though, and we will use that as our setting in the final KNN model.

As we have done a few times now, we will check our performance visually. This helps us see how well the model is predicting on different regions of actual values. We will get predictions from our knn model using the .predict() method on our scaled features. Then we'll use matplotlib's plt.scatter() to create a scatter plot of actual versus predicted values. 

# Create the model with the best-performing n_neighbors of 5
knn = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knn.fit(scaled_train_features, train_targets)

# Get predictions for train and test sets
train_predictions = knn.predict(scaled_train_features)
test_predictions = knn.predict(scaled_test_features)

# Plot the actual vs predicted values
plt.scatter(train_predictions, train_targets, label='train')
plt.scatter(test_predictions, test_targets, label='test')
plt.legend()
plt.show() 


Neural networks have potential 
Neural nets have:

non-linearity
variable interactions
customizability


implementing a neural net with keras 

from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(50,
                input_dim=scaled_train_features.shape[1],
                activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='linear')) 

Fitting the model
model.compile(optimizer='adam', loss='mse')
history = model.fit(scaled_train_features, train_targets, epochs=50) 

Examining the loss
plt.plot(history.history['loss'])
plt.title('loss:' + str(round(history.history['loss'][-1], 6)))
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

Checking out performance
from sklearn.metrics import r2_score

# calculate R^2 score
train_preds = model.predict(scaled_train_features)
print(r2_score(train_targets, train_preds))
 0.4771387560719418 

Build and fit a simple neural net
The next model we will learn how to use is a neural network. Neural nets can capture complex interactions between variables, but are difficult to set up and understand. Recently, they have been beating human experts in many fields, including image recognition and gaming (check out AlphaGo) -- so they have great potential to perform well.

To build our nets we'll use the keras library. This is a high-level API that allows us to quickly make neural nets, yet still exercise a lot of control over the design. The first thing we'll do is create almost the simplest net possible -- a 3-layer net that takes our inputs and predicts a single value. Much like the sklearn models, keras models have a .fit() method that takes arguments of (features, targets).

from keras.models import Sequential
from keras.layers import Dense

# Create the model
model_1 = Sequential()
model_1.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))
model_1.add(Dense(20, activation='relu'))
model_1.add(Dense(1, activation='linear'))

# Fit the model
model_1.compile(optimizer='adam', loss='mse')
history = model_1.fit(scaled_train_features, train_targets, epochs=25) 

Epoch 1/25

 32/250 [==>...........................] - ETA: 1s - loss: 0.0492
250/250 [==============================] - 0s 1ms/step - loss: 0.0225
Epoch 2/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0094
250/250 [==============================] - 0s 82us/step - loss: 0.0071
Epoch 3/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0062
250/250 [==============================] - 0s 80us/step - loss: 0.0047
Epoch 4/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0054
250/250 [==============================] - 0s 81us/step - loss: 0.0029
Epoch 5/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0029
250/250 [==============================] - 0s 82us/step - loss: 0.0024
Epoch 6/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0028
250/250 [==============================] - 0s 81us/step - loss: 0.0018
Epoch 7/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0015
250/250 [==============================] - 0s 84us/step - loss: 0.0016
Epoch 8/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0016
250/250 [==============================] - 0s 82us/step - loss: 0.0013
Epoch 9/25

 32/250 [==>...........................] - ETA: 0s - loss: 9.4421e-04
250/250 [==============================] - 0s 80us/step - loss: 0.0012
Epoch 10/25

 32/250 [==>...........................] - ETA: 0s - loss: 9.5193e-04
250/250 [==============================] - 0s 83us/step - loss: 0.0011
Epoch 11/25

 32/250 [==>...........................] - ETA: 0s - loss: 8.3738e-04
250/250 [==============================] - 0s 80us/step - loss: 0.0010
Epoch 12/25

 32/250 [==>...........................] - ETA: 0s - loss: 9.5945e-04
250/250 [==============================] - 0s 81us/step - loss: 9.0909e-04
Epoch 13/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.3269e-04
250/250 [==============================] - 0s 81us/step - loss: 8.4439e-04
Epoch 14/25

 32/250 [==>...........................] - ETA: 0s - loss: 6.6026e-04
250/250 [==============================] - 0s 79us/step - loss: 7.8474e-04
Epoch 15/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.4951e-04
250/250 [==============================] - 0s 81us/step - loss: 7.4006e-04
Epoch 16/25

 32/250 [==>...........................] - ETA: 0s - loss: 9.2834e-04
250/250 [==============================] - 0s 80us/step - loss: 7.0401e-04
Epoch 17/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.0727e-04
250/250 [==============================] - 0s 82us/step - loss: 6.5693e-04
Epoch 18/25

 32/250 [==>...........................] - ETA: 0s - loss: 3.9313e-04
250/250 [==============================] - 0s 80us/step - loss: 6.2054e-04
Epoch 19/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.0258e-04
250/250 [==============================] - 0s 81us/step - loss: 6.1881e-04
Epoch 20/25

 32/250 [==>...........................] - ETA: 0s - loss: 3.6411e-04
250/250 [==============================] - 0s 81us/step - loss: 5.6777e-04
Epoch 21/25

 32/250 [==>...........................] - ETA: 0s - loss: 2.9391e-04
250/250 [==============================] - 0s 80us/step - loss: 5.5606e-04
Epoch 22/25

 32/250 [==>...........................] - ETA: 0s - loss: 5.0603e-04
250/250 [==============================] - 0s 81us/step - loss: 5.2693e-04
Epoch 23/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.0897e-04
250/250 [==============================] - 0s 83us/step - loss: 4.8523e-04
Epoch 24/25

 32/250 [==>...........................] - ETA: 0s - loss: 4.5235e-04
250/250 [==============================] - 0s 171us/step - loss: 4.9395e-04
Epoch 25/25

 32/250 [==>...........................] - ETA: 0s - loss: 1.8438e-04
250/250 [==============================] - 0s 173us/step - loss: 4.6491e-04

Plot losses
Once we've fit a model, we usually check the training loss curve to make sure it's flattened out. The history returned from model.fit() is a dictionary that has an entry, 'loss', which is the training loss. We want to ensure this has more or less flattened out at the end of our training. 

# Plot the losses from the fit
plt.plot(history.history['loss'])

# Use the last loss as the title
plt.title('loss:' + str(round(history.history['loss'][-1], 6)))
plt.show() 

Measure performance
Now that we've fit our neural net, let's check performance to see how well our model is predicting new values. There's not a built-in .score() method like with sklearn models, so we'll use the r2_score() function from sklearn.metrics. This calculates the R2 score given arguments (y_true, y_predicted). We'll also plot our predictions versus actual values again. This will yield some interesting results soon (once we implement our own custom loss function). 

from sklearn.metrics import r2_score

# Calculate R^2 score
train_preds = model_1.predict(scaled_train_features)
test_preds = model_1.predict(scaled_test_features)
print(r2_score(train_targets, train_preds))
print(r2_score(test_targets, test_preds))

# Plot predictions vs actual
plt.scatter(train_preds, train_targets, label='train')
plt.scatter(test_preds, test_targets, label='test')
plt.legend()
plt.show() 

Mean squared error loss
import tensorflow as tf

# create loss function
def mean_squared_error(y_true, y_pred):
    loss = tf.square(y_true - y_pred)
    return tf.reduce_mean(loss, axis=-1) 

Add custom loss to keras
import tensorflow as tf

# create loss function
def mean_squared_error(y_true, y_pred):
    loss = tf.square(y_true - y_pred)
    return tf.reduce_mean(loss, axis=-1)
# enable use of loss with keras
import keras.losses
keras.losses.mean_squared_error = mean_squared_error 

# fit the model with our mse loss function
model.compile(optimizer='adam', loss=mean_squared_error)
history = model.fit(scaled_train_features, train_targets, epochs=50) 


Using tf.where()
# create loss function
def sign_penalty(y_true, y_pred):
    penalty = 10.
    loss = tf.where(tf.less(y_true * y_pred, 0), \
                     penalty * tf.square(y_true - y_pred), \
                     tf.square(y_true - y_pred)) 

Tying it together
# create loss function
def sign_penalty(y_true, y_pred):
    penalty = 100.
    loss = tf.where(tf.less(y_true * y_pred, 0), \
                     penalty * tf.square(y_true - y_pred), \
                     tf.square(y_true - y_pred))

    return tf.reduce_mean(loss, axis=-1)
keras.losses.sign_penalty = sign_penalty  # enable use of loss with keras 


Using the custom loss
# create the model
model = Sequential()
model.add(Dense(50,
                input_dim=scaled_train_features.shape[1],
                activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='linear'))
 # fit the model with our custom 'sign_penalty' loss function
model.compile(optimizer='adam', loss=sign_penalty)
history = model.fit(scaled_train_features, train_targets, epochs=50) 

The bow-tie shape
train_preds = model.predict(scaled_train_features)

# scatter the predictions vs actual
plt.scatter(train_preds, train_targets)
plt.xlabel('predictions')
plt.ylabel('actual')
plt.show() 

Custom loss function
Up to now, we've used the mean squared error as a loss function. This works fine, but with stock price prediction it can be useful to implement a custom loss function. A custom loss function can help improve our model's performance in specific ways we choose. For example, we're going to create a custom loss function with a large penalty for predicting price movements in the wrong direction. This will help our net learn to at least predict price movements in the correct direction.

To do this, we need to write a function that takes arguments of (y_true, y_predicted). We'll also use functionality from the backend keras (using tensorflow) to find cases where the true value and prediction don't match signs, then penalize those cases.

import keras.losses
import tensorflow as tf

# Create loss function
def sign_penalty(y_true, y_pred):
    penalty = 100.
    loss = tf.where(tf.less(y_true * y_pred, 0), \
                     penalty * tf.square(y_true - y_pred), \
                     tf.square(y_true - y_pred))

    return tf.reduce_mean(loss, axis=-1)

keras.losses.sign_penalty = sign_penalty  # enable use of loss with keras
print(keras.losses.sign_penalty) 

Fit neural net with custom loss function
Now we'll use the custom loss function we just created. This will enable us to alter the model's behavior in useful ways particular to our problem -- it's going to try to force the model to learn how to at least predict price movement direction correctly. All we need to do now is set the loss argument in our .compile() function to our function name, sign_penalty. We'll examine the training loss again to make sure it's flattened out. 

# Create the model
model_2 = Sequential()
model_2.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))
model_2.add(Dense(20, activation='relu'))
model_2.add(Dense(1, activation='linear'))

# Fit the model with our custom 'sign_penalty' loss function
model_2.compile(optimizer='adam', loss=sign_penalty)
history = model_2.fit(scaled_train_features, train_targets, epochs=25)
plt.plot(history.history['loss'])
plt.title('loss:' + str(round(history.history['loss'][-1], 6)))
plt.show() 

Epoch 1/25

 32/250 [==>...........................] - ETA: 1s - loss: 3.9555
250/250 [==============================] - 0s 1ms/step - loss: 1.6141
Epoch 2/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.6545
250/250 [==============================] - 0s 86us/step - loss: 0.4220
Epoch 3/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.5371
250/250 [==============================] - 0s 82us/step - loss: 0.2793
Epoch 4/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.3604
250/250 [==============================] - 0s 82us/step - loss: 0.1812
Epoch 5/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.1763
250/250 [==============================] - 0s 82us/step - loss: 0.1323
Epoch 6/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.2079
250/250 [==============================] - 0s 82us/step - loss: 0.1093
Epoch 7/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0525
250/250 [==============================] - 0s 82us/step - loss: 0.0836
Epoch 8/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0427
250/250 [==============================] - 0s 115us/step - loss: 0.0725
Epoch 9/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0355
250/250 [==============================] - 0s 152us/step - loss: 0.0584
Epoch 10/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0289
250/250 [==============================] - 0s 154us/step - loss: 0.0540
Epoch 11/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0506
250/250 [==============================] - 0s 146us/step - loss: 0.0476
Epoch 12/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0550
250/250 [==============================] - 0s 184us/step - loss: 0.0372
Epoch 13/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0360
250/250 [==============================] - 0s 145us/step - loss: 0.0364
Epoch 14/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0366
250/250 [==============================] - 0s 107us/step - loss: 0.0361
Epoch 15/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0222
250/250 [==============================] - 0s 82us/step - loss: 0.0227
Epoch 16/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0150
250/250 [==============================] - 0s 81us/step - loss: 0.0195
Epoch 17/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0143
250/250 [==============================] - 0s 81us/step - loss: 0.0168
Epoch 18/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0101
250/250 [==============================] - 0s 81us/step - loss: 0.0160
Epoch 19/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0038
250/250 [==============================] - 0s 82us/step - loss: 0.0109
Epoch 20/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0041
250/250 [==============================] - 0s 191us/step - loss: 0.0139
Epoch 21/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0207
224/250 [=========================>....] - ETA: 0s - loss: 0.0153
250/250 [==============================] - 0s 314us/step - loss: 0.0142
Epoch 22/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0243
250/250 [==============================] - 0s 150us/step - loss: 0.0136
Epoch 23/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0081
250/250 [==============================] - 0s 127us/step - loss: 0.0065
Epoch 24/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0040
192/250 [======================>.......] - ETA: 0s - loss: 0.0116
250/250 [==============================] - 0s 293us/step - loss: 0.0117
Epoch 25/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0060
224/250 [=========================>....] - ETA: 0s - loss: 0.0082
250/250 [==============================] - 0s 263us/step - loss: 0.0095


Visualize the results
We've fit our model with the custom loss function, and it's time to see how it is performing. We'll check the R2 values again with sklearn's r2_score() function, and we'll create a scatter plot of predictions versus actual values with plt.scatter(). This will yield some interesting results!

# Evaluate R^2 scores
train_preds = model_2.predict(scaled_train_features)
test_preds = model_2.predict(scaled_test_features)
print(r2_score(train_targets, train_preds))
print(r2_score(test_targets, test_preds))

# Scatter the predictions vs actual -- this one is interesting!
plt.scatter(train_preds, train_targets, label='train')
plt.scatter(test_preds, test_targets, label='test')  # plot test set
plt.legend(); plt.show() 



Overfitting and ensembling

Dropout in keras
from keras.layers import Dense, Dropout
model = Sequential()
model.add(Dense(500,
                input_dim=scaled_train_features.shape[1],
                activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='linear'))

Implementing ensembling
# make predictions from 2 neural net models
test_pred1 = model_1.predict(scaled_test_features)
test_pred2 = model_2.predict(scaled_test_features) 

# horizontally stack predictions and take the average across rows
test_preds = np.mean(np.hstack((test_pred1, test_pred2)), axis=1) 


Combatting overfitting with dropout
A common problem with neural networks is they tend to overfit to training data. What this means is the scoring metric, like R2 or accuracy, is high for the training set, but low for testing and validation sets, and the model is fitting to noise in the training data.

We can work towards preventing overfitting by using dropout. This randomly drops some neurons during the training phase, which helps prevent the net from fitting noise in the training data. keras has a Dropout layer that we can use to accomplish this. We need to set the dropout rate, or fraction of connections dropped during training time. This is set as a decimal between 0 and 1 in the Dropout() layer.

We're going to go back to the mean squared error loss function for this model. 

from keras.layers import Dropout

# Create model with dropout
model_3 = Sequential()
model_3.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))
model_3.add(Dropout(0.2))
model_3.add(Dense(20, activation='relu'))
model_3.add(Dense(1, activation='linear'))

# Fit model with mean squared error loss function
model_3.compile(optimizer='adam', loss='mse')
history = model_3.fit(scaled_train_features, train_targets, epochs=25)
plt.plot(history.history['loss'])
plt.title('loss:' + str(round(history.history['loss'][-1], 6)))
plt.show() 

Epoch 1/25

 32/250 [==>...........................] - ETA: 1s - loss: 0.0739
250/250 [==============================] - 0s 1ms/step - loss: 0.0640
Epoch 2/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0544
250/250 [==============================] - 0s 89us/step - loss: 0.0286
Epoch 3/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0185
250/250 [==============================] - 0s 90us/step - loss: 0.0155
Epoch 4/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0177
250/250 [==============================] - 0s 89us/step - loss: 0.0184
Epoch 5/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0142
250/250 [==============================] - 0s 89us/step - loss: 0.0135
Epoch 6/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0220
250/250 [==============================] - 0s 90us/step - loss: 0.0118
Epoch 7/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0172
250/250 [==============================] - 0s 93us/step - loss: 0.0109
Epoch 8/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0050
250/250 [==============================] - 0s 89us/step - loss: 0.0068
Epoch 9/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0072
250/250 [==============================] - 0s 90us/step - loss: 0.0083
Epoch 10/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0059
250/250 [==============================] - 0s 90us/step - loss: 0.0068
Epoch 11/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0066
250/250 [==============================] - 0s 96us/step - loss: 0.0060
Epoch 12/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0071
250/250 [==============================] - 0s 90us/step - loss: 0.0080
Epoch 13/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0125
250/250 [==============================] - 0s 90us/step - loss: 0.0067
Epoch 14/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0067
250/250 [==============================] - 0s 91us/step - loss: 0.0065
Epoch 15/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0034
250/250 [==============================] - 0s 175us/step - loss: 0.0058
Epoch 16/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0043
250/250 [==============================] - 0s 102us/step - loss: 0.0041
Epoch 17/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0031
250/250 [==============================] - 0s 116us/step - loss: 0.0050
Epoch 18/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0040
250/250 [==============================] - 0s 129us/step - loss: 0.0041
Epoch 19/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0069
250/250 [==============================] - 0s 101us/step - loss: 0.0051
Epoch 20/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0045
250/250 [==============================] - 0s 103us/step - loss: 0.0039
Epoch 21/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0050
250/250 [==============================] - 0s 118us/step - loss: 0.0037
Epoch 22/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0031
250/250 [==============================] - 0s 94us/step - loss: 0.0038
Epoch 23/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0026
250/250 [==============================] - 0s 90us/step - loss: 0.0031
Epoch 24/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0036
250/250 [==============================] - 0s 89us/step - loss: 0.0029
Epoch 25/25

 32/250 [==>...........................] - ETA: 0s - loss: 0.0030
250/250 [==============================] - 0s 90us/step - loss: 0.0027

Ensembling models
One approach to improve predictions from machine learning models is ensembling. A basic approach is to average the predictions from multiple models. A more complex approach is to feed predictions of models into another model, which makes final predictions. Both approaches usually improve our overall performance (as long as our individual models are good). If you remember, random forests are also using ensembling of many decision trees.

To ensemble our neural net predictions, we'll make predictions with the 3 models we just created -- the basic model, the model with the custom loss function, and the model with dropout. Then we'll combine the predictions with numpy's .hstack() function, and average them across rows with np.mean(predictions, axis=1). 

# Make predictions from the 3 neural net models
train_pred1 = model_1.predict(scaled_train_features)
test_pred1 = model_1.predict(scaled_test_features)

train_pred2 = model_2.predict(scaled_train_features)
test_pred2 = model_2.predict(scaled_test_features)

train_pred3 = model_3.predict(scaled_train_features)
test_pred3 = model_3.predict(scaled_test_features)

# Horizontally stack predictions and take the average across rows
train_preds = np.mean(np.hstack((train_pred1, train_pred2, train_pred3)), axis=1)
test_preds = np.mean(np.hstack((test_pred1, test_pred2, test_pred3)), axis=1)
print(test_preds[-5:]) 

See how the ensemble performed
Let's check performance of our ensembled model to see how it's doing. We should see roughly an average of the R2 scores, as well as a scatter plot that is a mix of our previous models' predictions. The bow-tie shape from the custom loss function model should still be a bit visible, but the edges near x=0 should be softer.

from sklearn.metrics import r2_score

# Evaluate the R^2 scores
print(r2_score(train_targets, train_preds))
print(r2_score(test_targets, test_preds))

# Scatter the predictions vs actual -- this one is interesting!
plt.scatter(train_preds, train_targets, label='train')
plt.scatter(test_preds, test_targets, label='test')
plt.legend(); plt.show() 






























